{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSFS12 Hand-in exercise 5, extra assignment, solution: Learning predictive driver models with neural networks\n",
    "Erik Frisk (erik.frisk@liu.se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is based on data from the I-80 data set from the U.S. Department of Transportation. The data can be downloaded from the course directory in \n",
    "Lisam, and are available in the directory /courses/tsfs12/i80_data in the student labs at campus. \n",
    "\n",
    "I-80 data set citation: U.S. Department of Transportation Federal Highway Administration. (2016). Next Generation Simulation (NGSIM) Vehicle\n",
    "Trajectories and Supporting Data. [Dataset]. Provided by ITS DataHub through Data.transportation.gov. Accessed 2020-09-29 from http://doi.org/10.21949/1504477. More details about the data set are \n",
    "available through this link.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make initial imports. The exercise requires python packages numpy, tensorflow, scikit-learn, and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from i80_utility import plot_road, lane_bounds, plot_prediction, load_i80_features, load_i80_trajectories, get_trajectory_from_datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The raw data used in the exercise is available from https://www.its.dot.gov/data/, US Department of Transportation, Intelligent Transport Systems datahub. More specifically, we will use the I80 data from the NGSIM program. The data was collected through a network of synchronized digital video cameras and then transcribed to vehicle trajectory data from the video. This vehicle trajectory data provided the precise location of each vehicle within the study area in 10 Hz, resulting in detailed lane positions and locations relative to other vehicles.\n",
    "\n",
    "https://data.transportation.gov/Automobiles/Next-Generation-Simulation-NGSIM-Vehicle-Trajector/8ect-6jqj\n",
    "\n",
    "The raw data is described in the file ```I-80_Metadata_Documentation.pdf```. There are predefined functions for reading the raw data (and units are converted to SI-units).\n",
    "\n",
    "From the raw trajectory data, we have designed features to be able to build predictive models. The data needed for this exercise can be downloaded from Lisam, thus you _do not_ have to download anything outside of Lisam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define where the data resides, on your computer or if you are working in the student labs. The variable ```i80_data_dir``` points to the directory where the data directory ```i80_data``` is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i80_data_dir = './'  # data downloaded in the current directory\n",
    "# i80_data_dir = '/courses/tsfs12/'  # student labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random number generator (with a specified seed so that results are reproducible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = np.random.default_rng(seed=1891)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load I-80 feature data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the I80 data, we have designed features to be able to build predictive models. Each datapointhas 41 features. The feature data consists of 95591 datapoints and consists of three variables:\n",
    "* x - The feature data, a (95591 x 41)-matrix.\n",
    "* y - True label for each datapoint.\n",
    "* info - Information which trajectory, dataset, and time-stamp the datapoint corresponds to.\n",
    "\n",
    "The feature data is described in more detail in the handin documentation and the file ```features.md```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, info = load_i80_features(i80_data_dir)\n",
    "print(f\"Read {x.shape[0]} datapoints with {x.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how many datapoints correspond to switching lane left, right, and staying in the same lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Left: {np.sum(y == 0)}, Straight: {np.sum(y == 1)}, Right: {np.sum(y == 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor out validation data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a validation data-set and carefully ensure that there is no leakage of validation datapoints into the training dataset. First, collect indices for all datapoints corresponding to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_class_idx = np.argwhere(y == 0).reshape(-1)\n",
    "straight_class_idx = np.argwhere(y == 1).reshape(-1)\n",
    "right_class_idx = np.argwhere(y == 2).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By random, select M datapoints from each class to be included in the dataset. The validation dataset will then be balanced. Due to the large imbalance, we can't include too many datapoints from each class, then very few would be available for training. Experiment with this number M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 50\n",
    "val_class_idx = (rg.choice(left_class_idx, M, replace=False),\n",
    "                 rg.choice(straight_class_idx, M, replace=False),\n",
    "                 rg.choice(right_class_idx, M, replace=False))\n",
    "validation_index = np.hstack(val_class_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance training data by resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the severe class imbalance in data, some measure need to be taken. Here, data is balanced by oversampling the underrepresented classes to weigh thios datapoints higher. The code below samples M datapoints, _with replacement_, from each class (excluding the validation data). Experiment also with the number M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1500  # Number of samples from each class\n",
    "train_class_idx = (rg.choice(np.setdiff1d(left_class_idx, val_class_idx[0]), M),\n",
    "                   rg.choice(np.setdiff1d(straight_class_idx, val_class_idx[1]), M),\n",
    "                   rg.choice(np.setdiff1d(right_class_idx, val_class_idx[2]), M))\n",
    "\n",
    "train_index = np.hstack(train_class_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect data points and lables for traing and validation in arrays ```x_train```, ```y_train```, ```x_val```, ```y_val```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[train_index]\n",
    "y_train = y[train_index]\n",
    "x_val = x[validation_index]\n",
    "y_val = y[validation_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step befor building models, normalize data so that each feature has mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(x_train, axis=0)\n",
    "std = np.std(x_train, axis=0)\n",
    "\n",
    "x_val = (x_val - mu) / std\n",
    "x_train = (x_train - mu) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to normalize data also when doing predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulate model and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will not need any advanced architectures in this exercise. Start with the \"Hello World\" example at https://www.tensorflow.org/overview and adapt to the lane-change prediction model in this exercise. Try to experiment also with regularization techniques, e.g., ```tf.keras.layers.Dropout```layers (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(20, input_shape=(#YOUR_CODE_HERE,), activation=#YOUR_CODE_HERE),\n",
    "    #YOUR_CODE_HERE\n",
    "    tf.keras.layers.Dense(#YOUR_CODE_HERE, activation=#YOUR_CODE_HERE)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=30, validation_data=(x_val, y_val))\n",
    "\n",
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss and accuracy for test and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(10, clear=True)\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "\n",
    "plt.figure(11, clear=True)\n",
    "plt.plot(hist.history['accuracy'], label='train')\n",
    "plt.plot(hist.history['val_accuracy'], label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a random datapoint from the validation dataset and make a prediction and compare with true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_index = rg.choice(validation_index, 1)\n",
    "yhat = model.predict((x[xi_index] - mu) / std)\n",
    "print(f\"Prediction: {yhat}\")\n",
    "print(f\"True label: {int(y[xi_index][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the confusion matrix for training and validation data using the imported ```confusion_matrix``` function. Function ```np.argmax``` can also be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?confusion_matrix  # Run this for help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_train = 0  #YOUR_CODE_HERE\n",
    "print(C_train)\n",
    "\n",
    "C_val = 0  # YOUR_CODE_HERE\n",
    "print(C_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple visualization of model predictions given the vehicle trajectories. First, load all trajectories from the I-80 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = load_i80_trajectories(i80_data_dir)\n",
    "\n",
    "print(f\"0400pm-0415pm: {len(trajectories[0])} trajectories.\")\n",
    "print(f\"0500pm-0515pm: {len(trajectories[1])} trajectories.\")\n",
    "print(f\"0515pm-0530pm: {len(trajectories[2])} trajectories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trajectories are stores as pandas dataframes. For example, the first samples of the first trajectory in the first data set (0400pm-0415pm) has the following data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories[0][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot N=100 random trajectories from the first data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plt.figure(10, clear=True)\n",
    "for trajectory_idx in rg.choice(range(len(trajectories[0])), N):\n",
    "    trajectory = trajectories[0][trajectory_idx]\n",
    "    plt.plot(trajectory.Local_X, trajectory.Local_Y, color=colors[trajectory.Lane_ID.iloc[0]], lw=0.5)\n",
    "plot_road()\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot random trajectories from the validation dataset. The function ```get_trajectory_from_datapoint``` finds which trajectory contains the prediction point, and also returns the index to all points on the trajectory included in the feature dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # Number of trajectories\n",
    "plt.figure(30, clear=True)\n",
    "plot_road()\n",
    "for val_index in rg.choice(validation_index, N):\n",
    "    trajectory, _ = get_trajectory_from_datapoint(val_index, info, trajectories)\n",
    "    plt.plot(trajectory.Local_X, trajectory.Local_Y, color=colors[trajectory.Lane_ID.iloc[0]], lw=0.5)\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a point by random from the validation dataset, find the corresponding trajectory, and make predictions along the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_i = rg.choice(validation_index)\n",
    "trajectory, data_points = get_trajectory_from_datapoint(val_i, info, trajectories)\n",
    "\n",
    "plt.figure(40, clear=True)\n",
    "plot_road()\n",
    "plt.plot(trajectory.Local_X, trajectory.Local_Y)\n",
    "for ti, xi in zip(info[data_points][:, 1], x[data_points]):\n",
    "    x_norm = (xi - mu) / std\n",
    "    lane_change_prediction = model.predict(x_norm[None, :])[0]\n",
    "    \n",
    "    pos_prediction = (trajectory.Local_X.iloc[ti], trajectory.Local_Y.iloc[ti])\n",
    "    plot_prediction(pos_prediction, lane_change_prediction, lane_bounds)\n",
    "\n",
    "    plt.plot(trajectory.Local_X.iloc[ti], trajectory.Local_Y.iloc[ti], 'ro')\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
