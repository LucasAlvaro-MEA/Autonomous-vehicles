{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSFS12 Hand-in exercise 5: Learning for autonomous vehicles - Reinforcement learning and Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rl_auxiliary import plot_iter, next_state, BoxOff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterize the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "R_goal = 0.0 # Reward for reaching goal state\n",
    "R_sink = -10.0  # Reward for reaching 'cliff' states\n",
    "R_grid = -0.1 # Reward for remaining states\n",
    "alpha = 0.5  # learning rate in Q-update\n",
    "eps = 0.5  # epsilon-greedy parameter\n",
    "\n",
    "P_move_action = 1.0  # probability of moving in the direction specified by action\n",
    "P_dist = (1-P_move_action)/2  # probability of moving sideways compared to intended because of disturbance\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 5\n",
    "\n",
    "# Set start and goal/sink positions.\n",
    "goal = np.array([3, 4])  # element index goal state\n",
    "sink = np.array([[3, 1], [3, 2], [3, 3]])  # element indicies for cliff states\n",
    "\n",
    "# Setup reward matrix R\n",
    "R = np.full((n_rows, n_cols), fill_value=R_grid)\n",
    "R[goal[0], goal[1]] = R_goal\n",
    "R[sink[:, 0], sink[:, 1]] = R_sink\n",
    "\n",
    "occ_grid = np.zeros((n_rows, n_cols))\n",
    "occ_grid[1, 1] = 1\n",
    "\n",
    "# Save parameters in a dictionary\n",
    "params = {'gamma': gamma, 'R_goal': R_goal, 'R_sink': R_sink,\n",
    "          'alpha': alpha, 'eps': eps,\n",
    "          'R_grid': R_grid, 'P_move_action': P_move_action, \n",
    "          'P_dist': P_dist, 'n_rows': n_rows, 'n_cols': n_cols, \n",
    "          'goal': goal, 'sink': sink, 'R': R, 'occ_grid': occ_grid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_eps_greedy(s_curr, k, Q, params):\n",
    "    \"\"\"Selection the action to take using a epsilon-greedy strategy\n",
    "    \n",
    "      action = select_eps_greedy(s_curr, k, Q, params):\n",
    "      \n",
    "      Input:\n",
    "          s_curr - current satte\n",
    "          k - Current iteration number\n",
    "          Q - Q matrix\n",
    "          params - parameter dictionary\n",
    "          \n",
    "      Output:\n",
    "          action - selected action\n",
    "    \"\"\"\n",
    "    eps = params['eps']\n",
    "\n",
    "    rnd = np.random.uniform()\n",
    "    max_a = np.argmax(Q[s_curr[0], s_curr[1]])\n",
    "\n",
    "    a_list = []\n",
    "    for a in range(4):\n",
    "        if not a == max_a:\n",
    "            a_list.append(a)\n",
    "\n",
    "    if rnd < 1-eps+eps/4:\n",
    "        action = max_a\n",
    "    elif rnd < 1-eps+eps/2:\n",
    "        action = a_list[0]\n",
    "    elif rnd < 1-eps+3*eps/4:\n",
    "        action = a_list[1]\n",
    "    else:\n",
    "        action = a_list[2]\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize main learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value function for each state\n",
    "V = np.zeros((n_rows, n_cols))\n",
    "\n",
    "# Actions - ['left','right','up','down'] counted as 0-3\n",
    "\n",
    "# Initialize Q for terminal states to zero\n",
    "Q = np.random.uniform(size=(n_rows, n_cols, 4))  # Number of rows x number of columns x number of actions\n",
    "Q[goal[0], goal[1]] = 0.0\n",
    "for si in sink:\n",
    "    Q[si[0], si[1]] = 0.0\n",
    "    \n",
    "# Initialize vector for policy\n",
    "Pi = np.full((n_rows, n_cols), fill_value=-1)\n",
    "\n",
    "# Define number of iterations for Q-learning\n",
    "nbr_iters = 10000\n",
    "\n",
    "# Initialize for sum of rewards for each episode\n",
    "sum_r = np.zeros(nbr_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute iterations until convergance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged = False\n",
    "for k in range(nbr_iters):\n",
    "    # Start state\n",
    "    s_curr = [n_rows-1, 0]\n",
    "    \n",
    "    terminal_state = False\n",
    "    \n",
    "    while not terminal_state:\n",
    "        # Select action according to epsilon-greedy strategy\n",
    "        action = select_eps_greedy(s_curr, k, Q, params)\n",
    "        \n",
    "        # Perform the action and receive reward and next state\n",
    "        s_next,r = next_state(s_curr, action, params)\n",
    "        \n",
    "        # Q-learning update of action-value function\n",
    "        Q[s_curr[0], s_curr[1], action] = (\n",
    "            Q[s_curr[0], s_curr[1], action] + \n",
    "                alpha*(r + gamma*np.max(Q[s_next[0], s_next[1]]) - \n",
    "                Q[s_curr[0], s_curr[1], action]))\n",
    "        \n",
    "        # Update the sum of reward vector\n",
    "        sum_r[k] = sum_r[k] + r\n",
    "        \n",
    "        s_curr = s_next\n",
    "        \n",
    "        # Check if a terminal state has been reached (closes an episode)\n",
    "        if (np.all(s_curr==goal) or\n",
    "            np.any(np.logical_and(s_curr[0]==sink[:, 0], s_curr[1]==sink[:, 1]))):\n",
    "            terminal_state = True\n",
    "            \n",
    "            # Update value function and policy\n",
    "            for row in range(n_rows):\n",
    "                for col in range(n_cols):\n",
    "                    if ((occ_grid[row, col] == 1) or \n",
    "                        np.all([row, col]==goal) or\n",
    "                        np.any(np.logical_and(row==sink[:, 0], col==sink[:, 1]))):\n",
    "                        continue\n",
    "                    max_a = np.argmax(Q[row, col])\n",
    "                    V_ij = Q[row, col, max_a]\n",
    "                    V[row, col] = V_ij\n",
    "                    Pi[row, col] = max_a    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the value function and policy after all iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iter(V, Pi, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute average of reward for N episodes for smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40\n",
    "mean_sum_r = np.zeros(sum_r.shape[0])\n",
    "\n",
    "for k in range(N, sum_r.shape[0]):\n",
    "    mean_sum_r[k] = np.mean(sum_r[k-N:k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the evolution of the reward for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, clear=True)\n",
    "plt.plot(mean_sum_r[N:], lw=0.5)\n",
    "plt.title('Sum of rewards for each episode (average over {})'.format(N))\n",
    "BoxOff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
