{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSFS12 Hand-in exercise 5: Learning for autonomous vehicles - Reinforcement learning and value iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rl_auxiliary import V_update, plot_iter, BoxOff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters in value iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "R_goal = 0.0 # Reward for reaching goal state\n",
    "R_sink = -10.0  # Reward for reaching 'cliff' states\n",
    "R_grid = -0.1 # Reward for remaining states\n",
    "\n",
    "P_move_action = 0.99  # probability of moving in the direction specified by action\n",
    "P_dist = (1-P_move_action)/2  # probability of moving sideways compared to intended because of disturbance\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 5\n",
    "\n",
    "# Set start and goal/sink positions.\n",
    "goal = np.array([3, 4])  # element index goal state\n",
    "sink = np.array([[3, 1], [3, 2], [3, 3]])  # element indices for cliff states\n",
    "\n",
    "# Setup reward matrix R\n",
    "R = np.full((n_rows, n_cols), fill_value=R_grid)\n",
    "R[goal[0], goal[1]] = R_goal\n",
    "R[sink[:, 0], sink[:, 1]] = R_sink\n",
    "\n",
    "occ_grid = np.zeros((n_rows, n_cols))\n",
    "occ_grid[1, 1] = 1\n",
    "\n",
    "# Save parameters in a struct\n",
    "params = {'gamma': gamma, 'R_goal': R_goal, 'R_sink': R_sink, \n",
    "          'R_grid': R_grid, 'P_move_action': P_move_action, \n",
    "          'P_dist': P_dist, 'n_rows': n_rows, 'n_cols': n_cols, \n",
    "          'goal': goal, 'sink': sink, 'R': R, 'occ_grid': occ_grid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop for value iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop for value iterations. Press return to proceed to next iteration. Note that plots must be made in an external window, not inline, since value function and policy is updated in the `plot_iter` function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop for value iteration\n",
    "# Algorithm according to Section 4.4 in Sutton, R. S., & A. G. Barto: \n",
    "# Reinforcement learning: An introduction. MIT Press, 2018.\n",
    "\n",
    "# Initilaize value function for each state\n",
    "V = np.zeros((n_rows,n_cols))\n",
    "\n",
    "# Actions - ['left','right','up','down'] counted as 0-3\n",
    "\n",
    "# Initialize vector for policy\n",
    "Pi = np.full((n_rows, n_cols), fill_value=-1)\n",
    "converged = False\n",
    "\n",
    "while not converged:    \n",
    "    Delta = 0\n",
    "\n",
    "    for row in range(n_rows):\n",
    "        for col in range(n_cols):\n",
    "            if ((occ_grid[row, col] == 1) or \n",
    "               np.all([row, col]==goal) or\n",
    "               np.any(np.logical_and(row==sink[:, 0], col==sink[:, 1]))):\n",
    "                continue\n",
    "            v = V[row, col]\n",
    "            V[row, col], max_a = V_update(row, col, V, params)\n",
    "            Pi[row, col] = max_a\n",
    "            Delta = np.max((Delta, np.abs(v-V[row, col])))\n",
    "    \n",
    "    # Visualize current value function and associated actions according to\n",
    "    # current policy\n",
    "    plot_iter(V, Pi, params)\n",
    "    print(V)\n",
    "    print(Pi)\n",
    "    print('Press enter')\n",
    "    _ = input()\n",
    "    \n",
    "    if Delta < 1e-6:\n",
    "        converged = True\n",
    "print('Convergence!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
